{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습의 본질이라고 하면 환경(environment)과의 상호작용(interation)을 아마도 먼저 떠올리게 될 것이다. 일련의 행위(action)를 하는 과정에서 환경과의 상호작용에서 얻어지는 정보(행위에 대한 결과)를 바탕으로 우리는 목표를 성취할 수 있는 방법에 대해 점차 깨우쳐가게 된다. 이렇게 상호작용을 통해 배우는 것은 대부분의 learning and intelligence의 기저에 있는 발상이다.\n",
    "이 책에서는 상호작용 그리고 학습에 대해 계산적인 관점의 접근(computational approach)을 해보도록 한다. 그리고 그와 관련된 여러가지 기법들 중에서도 *reinforcement learning*이라 불리는, 목표지향적 상호작용 학습(goal-directed learning from interation)에 대해 알아볼 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning은 주어진 어떤 상황(situation)에서 보상(reward)을 최대화 할 수 있는 행동(action)에 대해 학습하는 것이다. 학습의 주체는 상황에 따른 가장 적합한 action을 사전에 알고있지 못하므로 이를 찾아가는 과정에서 수많은 시행착오를 겪게될 것이다. 또한 좀 더 복잡한 상황에서는 현재의 action이 미래의 순차적인 보상에 영향을 줄 수도 있다. Reinforcement learning에서는 이 두 가지 사항-*trial-and-error search* and *delayed reward*-을 가장 중요한 특성으로 뽑을 수 있다.\n",
    "\n",
    "Reinforcement learning의 문제는 incompletely-known Markov decision processes(MDP)로 정형화(formalize)된다. 여기서 MDP는 sensation, action, goal의 세 가지 개념을 포함하는데, 학습의 주체인 agent는 환경이 어떤 상태(state)인지 인지할 수 있어야하고, 주어진 상태에 따라 action을 선택할 수 있어야하며, 또한 목적(goal)을 가지고있어야 한다.\n",
    "\n",
    "Reinforcement learning은 *supervised learning*과 다른 범주에 속한다. Supervised learning에서는 어떤 도메인의 전문가로부터 training set을 제공받게 되며, 이 training set을 통해 학습함으로써 training set에 포함되지 않은 입력에 대해서도 적절한 출력을 보이게끔 기대한다. 하지만 reinforcement learning의 문제에서는 agent가 처할 수 있는 상황과 그에 대응되는 행동에 대해 정확하고 적절한 예시를 얻어내기가 결코 쉽지 않은 경우가 많다. 또한 Reinforcement learning은 *unsupervised learning*과도 다른 범주에 속한다. Unsupervised learning이 unlabelled data로부터 hidden structure를 찾는 것을 목표로 하는 것에 비해, reinforcement learning은 보상을 최대화시키는 것을 주요한 목표로 삼기 때문이다. 그러므로 reinforcement learning은 기계학습의 세 번째 패러다임으로 간주하도록 한다.\n",
    "\n",
    "Reinforcement learning에서의 커다란 과제중 하나는 exploration과 exploitation 사이에서의 적절한 균형점을 찾는 것이다. 더 높은 보상을 받기 위해서는 주어진 상황에서의 더 적절한 action을 선택(exploit)해야 하는데, 이때 각 action들의 가치에 대해 알기 위해서는 사전에 탐험(explore)을 하는 것이 필요하다. 특히 stochastic task에서는 expected reward에 대해 신뢰할 수 있는 추정을 하기까지 각 action들이 더욱 많이 선택되어야 하는데, 이렇게 탐험을 하기 위해서는 지금 당장 최선이라고 믿어지는 action을 때로는 포기할 수도 있어야한다. 우리는 이러한 exploration과 exploitation의 관계를 *exploration-exploitation dilemma*라고 부른다. 이는 reinforcement learning을 supervised learning, unsupervised learning과 구분짓는 또 한가지의 차이점이다.\n",
    "\n",
    "흥미로운 점은 modern reinforcement learning이 다른 공학 및 과학 분야들과 섞여 유의미한 결과물을 내고있다는 것이다. 가령, parameterized approximator를 이용한 reinforcement learning의 방법론들은 operations research and control theory에서의 \"[curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)\"를 해결하는데 이용되기도 한다. 또한 reinforcement learning의 많은 핵심 알고리즘들이 사람이나 동물들이 학습하는 과정과 유사한 점에 기인하여 심리학 및 뇌과학 분야에서 활발히 이용되기도 한다.\n",
    "\n",
    "1960년 말 이후로 많은 AI 연구자들은 general principle에 대한 연구 대신 어떤 도메인에서의 특정 문제들을 해결해보려는 방향으로 노선을 변경하였다. 그렇게 얻어낸 작은 기능들이 엄청나게 많이 모이게되면 그것이 결국 지능을 갖춘것과 동일한 것이리라는 가정에서다. 반면 최근의 인공지능 학계에서는 학습에 대한 general principles를 찾기위한 연구가 더욱 활발히 이루어지고 있고, reinforcement learning 또한 그러한 흐름을 함께하고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "적용가능한 예시나 응용사례들을 살펴보도록 하자.\n",
    "\n",
    "- 최고수 체스선수가 수를 두는 것. 각 위치와 말의 이동에 대한 즉각적이고 직관적인 판단 아래에 다음 수에 대한 결정을 내릴 수 있다.\n",
    "- 석유정제시설의 적응제어기가 실시간으로 파라미터를 조정하는 것. 제어기는 명시된 한계비용에 근거하여 산출량/비용/품질에 대한 적절한 균형점을 찾아야한다.\n",
    "- 새끼 가젤이 태어난 후 걷기위한 몸부림을 치는 것. 생후 30분만에 새끼 가젤은 시속 20마일로 뛸 수 있다.\n",
    "- 무선 청소로봇이 청소를 위해 방에 들어갈지 혹은 충전소로 돌아갈지 결정하는 것. 이는 현재 배터리의 잔량과 충전소에 도달하는데 걸렸던 시간에 대한 과거의 경험에 의거해 결정된다.\n",
    "\n",
    "위 예시들을 살펴보면 공통적으로 1. active decision making agent와 환경(environment) 사이의 상호작용(interaction)이 있고, 2. agent는 환경의 불확실성 속에서 목적(goal)을 달성하기 위해 동작하며, 3. agent의 행동(action)은 환경의 차후 상태(state)에 영향을 미침을 알 수 있다. 이때 행동에 의해 발생하는 결과는 완전히 예측될 수 없으므로 agent는 계속해서 환경을 관찰하고 적합한 반응을 해야한다. 또한 agent는 성능을 점차 향상시키기 위해 과거에 얻은 경험을 잘 활용할 수 있어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Elements of Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent와 environment 외에도 reinforcement learning system을 구성하는 4가지의 하위요소들이 있다. 바로 policy, reward signal, value function, 그리고 (optionally) 환경에 대한 모델(model)이다.\n",
    "\n",
    "1. **policy:** policy는 현재의 상태(state)에 대해 어떤 행동(action)을 결정하는 역할을 한다. 간단하게는 lookup table이 될 수도 있고, 복잡하게는 상당한 계산비용을 필요로하는 탐색과정이 될 수도 있다. 보통 policy는 확률적(stochastic)인 경우가 많다.\n",
    "2. **reward signal:** Agent가 어떤 행동(action)을 할 때마다 환경은 agent에게 숫자 하나를 보내주는데, 이 것이 바로 reward signal이다. Reinforcement learning의 목표는 reward의 총합을 최대화시키는 것이기 때문에 reward signal에 대한 정의가 바로 목표에 대한 정의라고 봐도 무방하다. 또한 action을 선택하고 reward signal이 발생하면 그 신호의 내용에 따라 방금 선택했던 action의 가치가 변동될 것이다. 즉, reward signal은 policy의 변경을 위한 우선적인 근거가 된다. 보통 reward signal은 state와 action에 대한 확률함수(stochastic function)이다.\n",
    "3. **value function:** reward signal이 지금 당장 얻게되는 보상이라고 한다면, value function은 좀 더 장기적인 관점에서의 가치를 의미한다. 대략적으로 말해서 state에 대한 value라고 하면, 그 state를 시작으로 agent가 얻게 되리라 예상되는 reward에 대한 총합이라고 할 수 있다. 즉, 어떤 state에서 지금 당장 받게되는 reward가 높지 않더라도, 뒤이어 만나게되는 state들이 높은 reward를 준다면 value는 높은 값을 갖게 될 것이다. Value는 이전에 발생한 reward들에 의해 계산되므로 reward가 없으면 value또 존재할 수 없다. 하지만 판단에 앞서 평가나 선택을 함에 있어서는 (가장 높은) value가 우선적으로 염두된다. Value를 어떻게 잘 산출해내는지에 대한 문제는 reinforcement learning에서 아주 중요하고도 어려운 주제에 속한다.\n",
    "4. **model:** model은 environment의 behavior를 추론하는 무언가이다. 예를들어, 어떤 state와 action이 주어졌을때 model은 (마치 환경이 그러하듯이) 그 결과로써 reward와 다음의 state를 반환한다. Model은 planning을 위해 사용되는데, 여기서 planning이란 일련의 action들에 의해 발생할 수 있는 상황들에 대해 직접 경험해보기 이전에 고려해보는 것을 칭한다. Reinforcement learning은 model과 planning을 이용하는 *model-based methods*와 그 반대의 개념인 (명시적인 시행착오에 의해 학습을 해나가는) *model-free methods*로 구분할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Limitations and Scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State는 policy와 value function에 대한 입력으로 사용되며 model에 대해서는 입력과 출력으로 사용된다. 다르게 말하자면 Reinforcement learning은 상태에 대해 굉장히 의존적이라고 할 수 있다. 이 책에서 state signal은 환경의 일부로 동작하는 어떤 preprocessing system에 의해 생성된다고 가정한다. 그러므로 state signal이 어떻게 생성되고, 변하며, 또 학습되는지에 대해서는 다루지 않을 것이다. 이는 무엇보다도 decision-making에 모든 초점을 맞추기 위함이다.\n",
    "\n",
    "이 책에서 다루는 대부분의 reinforcement learning methods는 value function을 추정하는 방법에 주안점을 두고 있다. 여기서 진화 알고리즘과 같은 *evolutionary methods*의 경우, value function을 추정하지 않더라도 reinforcement learning problem을 해결하는데에 사용할 수 있음을 유의하자. Policy에 대한 공간이 충분히 작고 구조화가 잘 되어있어서 좋은 policy를 찾기 쉬운 여건이라면 evolutionary methods들은 상당히 잘 동작할 수 있다. 하지만 evolutionary methods는 reinforcement learning problem에서의 유용한 구조를 활용하지 못한다. 가령, 적절한 policy를 찾는데 있어 state와 action의 관계가 상당히 중요한 경우를 예로 들 수 있다. 참고로 이 책에서는 evolutionary methods는 다루지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 An Extended Example: Tic-Tac-Toe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Tic-Tac-Toe](https://en.wikipedia.org/wiki/Tic-tac-toe) 게임을 예시로 reinforcement learning에 대해 좀 더 알아보자. 설명에 앞서서 한 가지 가정이 필요하다.\n",
    "\n",
    "- 숙련된 플레이어들끼리 Tic-Tac-Toe를 하게되면 항상 무승부가 된다. 그러므로 상대측 플레이어의 실력이 완벽하지 않음을 가정한다.\n",
    "\n",
    "위의 가정아래 승률을 최대화할 수 있는 플레이어를 만들어 볼 것이다. 일단 reinforcement learning을 사용하기에 앞서, 같은 문제에 대해 고전적인 여러 방법들을 적용해보면 어떨지 살펴보자.\n",
    "\n",
    "1. [Minimax](https://en.wikipedia.org/wiki/Minimax)는 최악의 상황에서 패배할 확률을 최소화시키는 컨셉이다. 즉, 위 가정에 의거한(상대방이 완벽하지 않은) agent를 만들기에는 적합하지 않다.\n",
    "2. [Dynamic Programming](https://en.wikipedia.org/wiki/Dynamic_programming)을 이용하기 위해서는 모든 상태에서 상대방이 어떤 수를 둘지에 대한 확률정보를 사전에 알고 있어야 한다.\n",
    "\n",
    "위 두 방법과 달리 [Evolutionary method](https://en.wikipedia.org/wiki/Evolutionary_algorithm)는 이 게임에서 사용될 수 있다. Evolutionary method는 가용한 모든 policies의 공간에서 승률이 가장 높은 policy를 탐색한다. 이 방법과 reinforcement learning과의 차이점에 대해서는 잠시 뒤에 다시 논해보기로 하자.\n",
    "\n",
    "Reinforcement learning으로 tic-tae-toe 문제를 풀기 위해서는 value function이 필요하다. Value function은 모든 가능한 states의 정보를 담고있는 table인데, 그 정보는 각각의 state에서 승리로 이어질 (가장 최근에 측정된) 확률(추정치)에 해당한다. 이때 그 추정치를 state에 대한 value라고 칭하고, 그 숫자에 따라 다음과 같은 해석을 하도록 한다.\n",
    "\n",
    "- State A가 State B보다 높은 value를 가지고 있으면 A가 B보다 더 좋은(better) State라고 판단한다.\n",
    "- Value가 0이면 필히 패함을 뜻하고, 1이면 무조건 승리, 0.5면 승패가 반반임을 뜻한다.\n",
    "\n",
    "추정된 value를 기반으로 대부분의 경우에는 가장 높은 value를 선택하는 것으로(greedily) 수를 두고, 아주 드문 경우에 한해 탐험(exploratory)을 위한 무작위의 수를 둔다. Value의 갱신을 함에 있어서는 아래의 그림과 같이 back up을 통해 최근의 state에서 과거의 state로 타고 올라가며 값을 조금씩 변경해주게 된다.\n",
    "\n",
    "<img src=\"images/backup.png\" alt=\"back-up\" style=\"width: 500px;\"/>\n",
    "<center>[Fig.1] $c^\\ast, g^\\ast$는 greedy action이고, $e$는 탐험을 의미한다.</center>\n",
    "\n",
    "각각의 갱신절차를 식으로 표현하면 다음과 같다.\n",
    "\n",
    "$$\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha \\big[ V(S_{t+1}) - V(S_t) \\big]\n",
    "$$\n",
    "\n",
    "여기서 $S_t$는 t 시점에서의 state를 나타내고, $\\alpha$는 0과 1사이의 아주 작은 수로 설정된다. 이 update rule은 *temporal-difference learning*의 한 예시로, 두 개의 연속한 시간에 대한 value의 차이를 반영하여 값을 변화시키는 방법이다. ($V(S_{t+1}) - V(S_t)$) 이때, $\\alpha$의 값이 시간이 지남에 따라 차츰 작아진다면 value는 점점 어떤 값으로 수렴해 갈 것이고, $\\alpha$의 값이 고정되어 있다면 value는 수렴되지 않고 상대방의 변화에 따라 계속하여 조정될 것이다. 이제 이 시점에서 reinforcement learning과 evolutionary method의 차이점에 대해 생각해보자. Evolutionary method가 승리시 해당 게임의 플레이에 관여한 모든 behavior에 대해 credit을 주는 반면, reinforcement learning은 각각의 state에 대해 각기 다른 value값을 추정함을 알 수 있다.\n",
    "\n",
    "이 간단한 예시는 reinforcement learning에 대한 몇 가지 주요한 특징을 보여준다.\n",
    "\n",
    "1. 환경(여기서는 상대방)과의 상호작용을 통해 학습한다.\n",
    "2. 확실한 목표(승리)가 있고, 미래에 대한 순차적인 영향을 고려한다. (back-up 참고)\n",
    "\n",
    "반면 예시의 간단함으로 인해 reinforcement learning의 특징이 제대로 표현되지 못한 부분이 있는데, 이는 다음과 같다.\n",
    "\n",
    "1. 두 플레이어의 대결구도가 아닌 예시에도 적용될 수 있다.\n",
    "2. 분리된 episode가 아닌 경우에도 적용가능하다. (Tic-tac-toe에서는 한 번의 게임이 하나의 분리된 episode를 의미한다.)\n",
    "3. Discrete time step이 아닌, continuous time step에도 적용될 수 있다.\n",
    "4. Large state set 또는 infinite state set에도 사용 가능하다. Gerry Tesauro는 약 $10^{20}$가지의 state에 달하는 [backgammon](https://en.wikipedia.org/wiki/Backgammon)이라는 게임을 강화학습으로 학습시키기 위해 artificial neural network를 사용했고, 인간 제일의 선수에게서 승리하는데 성공했다. (chapter 16)\n",
    "5. Tic-tac-toe에서는 게임에 대한 사전지식(prior knowldege)이 없었지만, efficient learning을 위해 사전지식이 활용될 수도 있다. (Sections 9.5, 17.4, and 13.1)\n",
    "6. 미리 가능한 수를 내다보고, 갸늠하기 위해 model을 이용할 수 있다. Model이 있는 방식을 model-based methods라 하고, model이 없는 방식을 model-free methods라고 한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Sutton, R. and Barto, A. (2018). *Reinforcement Learning: An Introduction*. 2nd ed. MIT Press"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
